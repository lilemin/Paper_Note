# Inception v1
## Motivation
提高神经网络性能的最直接方法，就是增加网络的深度，其中包括网络的levels，以及宽度（即每一个level的单元units数量）。但这会面临着两个主要缺陷：
  1. 更大更深的网络，意味着计算更多的参数，当训练集有限时，网络将会更容易过拟合
  2. 更大更深的网络，意味着耗费更多的计算资源

### Architectural Details
#### Idea
Inception的主要思想是，如何通过容易获得的密集组件来近似和覆盖卷积视觉网络的最佳局部稀疏结构。假设平移不变性意味着网络由convolutional building blocks构成并且在空间上不断重复该最佳局部稀疏结构。

[Provable bounds for learning some deep representations](https://arxiv.org/pdf/1310.6343.pdf)提出了一个层次性的结构，其中应该分析最后一层的相关统计，并聚类成具有高相关性的单元组。这些簇cluster形成下一层的单元，并连接到上一层中的单位。假设来自较前面的曾的每个单元对应输入图像的某些区域，且这些单元被分组为滤波器组filter banks。

在较底层的相关单元会集中在局部区域。许多簇cluters会集中在单个区域，其可以被下一层的1×1的卷基层覆盖[Network in network](https://arxiv.org/pdf/1312.4400.pdf)。此外，期待会有更少数量的空间分散的簇cluters可以被更大的卷积覆盖，在越来越大的区域上patches数量下降。为了避免patches对齐问题，Inception架构的当前版本仅限于卷积核尺寸为1×1,3×3和5×5;这个决定更多地基于方便，而不是必要性。这也意味着提出的架构是所有这些层的组合，其输出滤波器组连接成单个输出向量形成了下一阶段的输入。另外，由于池化操作对于目前卷积网络的成功至关重要，因此建议在每个这样的阶段添加一个替代的并行池化路径应该具有额外的有益效果（图2(a)）。
#### Problem
Inception模块的一个大问题是在于卷积层具有大量滤波器，即使适量的5×5卷积也可能是非常耗费计算资源。一旦池化单元添加到模块中，这个问题甚至会变得更明显：输出滤波器的数量等于前一阶段滤波器的数量。池化层输出和卷积层输出的合并会导致这一阶段到下一阶段输出数量不可避免的增加。虽然这种架构可能会覆盖最优稀疏结构，但它会非常低效，导致在几个阶段内计算量爆炸。
#### Slovement
在Inception模块中，明智地减少维度。低维嵌入可能包含较大图像块的信息。但以密集、压缩形式表示嵌入信息并且压缩信息是困难的。这种表示应该在大多数地方保持稀疏并且仅在它们必须汇总时才压缩信号。也就是说，在昂贵的3×3和5×5卷积之前，*1×1卷积用来计算降维。除了用来降维之外，它们也包括使用线性修正单元使其两用。最终的结果如图2(b)所示。

通常，Inception网络是一个由上述类型的模块互相堆叠组成的网络，偶尔会有步长为2的最大池化层将网络分辨率减半。出于技术原因（训练过程中内存效率），只在更高层开始使用Inception模块而在更低层仍保持传统的卷积形式似乎是有益的。

该架构的一个有用的方面是它允许显著增加每个阶段的单元数量，而不会在后面的阶段出现计算复杂度不受控制的爆炸。这是在尺寸较大的块进行昂贵的卷积之前通过普遍使用降维实现的。此外，设计遵循了实践直觉，即视觉信息应该在不同的尺度上处理然后聚合，为的是下一阶段可以从不同尺度同时抽象特征。

计算资源的改善使用允许增加每个阶段的宽度和阶段的数量，而不会陷入计算困境。可以利用Inception架构创建略差一些但计算成本更低的版本。
### GoogLeNet
所有的卷积都使用了修正线性激活，包括Inception模块内部的卷积。在网络中感受野是在均值为0的RGB颜色空间中，大小是224×224。“#3×3 reduce”和“#5×5 reduce”表示在3×3和5×5卷积之前，降维层使用的1×1滤波器的数量。在pool proj列可以看到内置的最大池化之后，投影层中1×1滤波器的数量。所有的这些降维/投影层也都使用了线性修正激活。

当只计算有参数的层时，网络有22层（如果我们也计算池化层是27层）。构建网络的全部层（独立构建块）的数目大约是100。确切的数量取决于机器学习基础设施对层的计算方式。分类器之前的平均池化是基于[12]的，尽管我们的实现有一个额外的线性层。线性层使我们的网络能很容易地适应其它的标签集，但它主要是为了方便使用，我们不期望它有重大的影响。我们发现从全连接层变为平均池化，提高了大约top-1 %0.6的准确率，然而即使在移除了全连接层之后，丢失的使用还是必不可少的。

给定深度相对较大的网络，有效传播梯度反向通过所有层的能力是一个问题。在这个任务上，更浅网络的强大性能表明网络中部层产生的特征应该是非常有识别力的。通过将辅助分类器添加到这些中间层，可以期望较低阶段分类器的判别力。这被认为是在提供正则化的同时克服梯度消失问题。这些分类器采用较小卷积网络的形式，放置在Inception (4a)和Inception (4b)模块的输出之上。在训练期间，它们的损失以折扣权重（辅助分类器损失的权重是0.3）加到网络的整个损失上。在推断时，这些辅助网络被丢弃。后面的控制实验表明辅助网络的影响相对较小（约0.5），只需要其中一个就能取得同样的效果.
  
辅助分类器家结构如下：
* 平均池化层采用5×5,步长为3的滤波器，在（4a）阶段输出4×4×512,在（4b）阶段输出4×4×528
* 1×1卷积层的滤波器数量是128,并且包含线性修正激活
* 全连接层有1024个神经元和线性修正激活
* dropout层的比例是70%
* 线性层和softmax分类器，最为主要分类器
![](images/inception_v1/inception_module.png) 
### Training Methodology
Item | Choice
----------- | ----------- 
optimator | asynchronous stochastic gradient descent with 0.9 momentum
learning rate | fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs)
Polyak averaging | create the final model
some tricks | image sample / photometric distortions of Andrew Howard [8]

## ILSVRC 2014 Classification Challenge Setup and Results
1. 独立训练同一GoogleLeNet的7个不同版本， 并用其进行预测。这7个模型训练期间采用相同的初始化方式（相同的初始化权重）和相同的学习率策略;进在图像采样方法和随机输入图像顺序。
2. 积极的图像裁剪方法。每个图像会得到4×3×6×2=144的裁剪图像。
* 对于每个图像，都归一化为四个尺度，其中较短的维度（高或宽）分别为256,288,320,352,取这些归一化图像的左、中、右方块在肖像图片中，我们采用顶部，中心和底部方块）
* 对于每个方块，采用4个角以及中心的224×224的裁剪图像以及该方块归一化为224×224
* 对于每个裁剪图像，都采用其镜像图像
* 需注意，存在合理数量的裁剪图像后，更多裁剪图像的好处会变得很微小
3. 不同裁剪图像和不同分类器的softmax概率取平均值，得到最终的预测结果。这方法比所有裁剪图像的max pooling和所有分类器的取平均的结果要好。

### ILSVRC 2014 Detection Challenge Setup and Results
